{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dog Breed Classification - Transfer Learning\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/)\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/)\n\n---\n\n## Project Overview\n\nIn this notebook, we leverage **Transfer Learning** — using models pretrained on ImageNet (1.2M images, 1000 classes) — to classify pet breeds from the **Oxford-IIIT Pet Dataset** (37 categories). We compare **four architectures**: ResNet-50, VGG-16, EfficientNet-B0, and MobileNet-V2, using a two-phase training strategy: frozen feature extraction followed by fine-tuning.\n\n**What you'll learn:**\n- Why transfer learning works and when to use it\n- Feature extraction vs fine-tuning strategies\n- How to adapt pretrained models for new tasks\n- Comparing model architectures systematically\n- ResNet skip connections, EfficientNet compound scaling, MobileNet depthwise separable convolutions\n\n---\n\n## Table of Contents\n\n1. [Imports](#1-imports)\n2. [Constants & Device Setup](#2-constants--device-setup)\n3. [Data Transforms](#3-data-transforms)\n4. [Dataset & DataLoader](#4-dataset--dataloader)\n5. [Visualize Data](#5-visualize-data)\n6. [Model Architectures](#6-model-architectures)\n7. [Display Model Summary](#7-display-model-summary)\n8. [Training Infrastructure](#8-training-infrastructure)\n9. [Training & Validation Functions](#9-training--validation-functions)\n10. [Train & Compare All Models](#10-train--compare-all-models)\n11. [Comprehensive Evaluation](#11-comprehensive-evaluation)\n12. [Save Best Model](#12-save-best-model)\n13. [Bonus: Stanford Dogs Extension](#13-bonus-stanford-dogs-extension)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup — install required packages (silent for Colab/Kaggle)\n",
    "!pip install torchinfo -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Imports\n",
    "\n",
    "### Why These Libraries?\n",
    "\n",
    "| Library | Purpose |\n",
    "|---|---|\n",
    "| **torch** | Core deep learning framework — tensors, autograd, neural network modules |\n",
    "| **torch.nn** | Pre-built neural network layers (Conv2d, Linear, BatchNorm, etc.) |\n",
    "| **torch.nn.functional** | Stateless functions (activation functions, loss functions) |\n",
    "| **torch.optim** | Optimization algorithms (SGD, Adam, learning rate schedulers) |\n",
    "| **torchvision** | Computer vision utilities — datasets, transforms, pretrained models |\n",
    "| **torchvision.models** | Pretrained model zoo (ResNet, VGG, EfficientNet, MobileNet, etc.) |\n",
    "| **torchinfo** | Model summary — parameter counts, layer shapes, memory usage |\n",
    "| **tqdm** | Progress bars for training loops |\n",
    "| **sklearn.metrics** | Classification report, confusion matrix, top-k accuracy |\n",
    "| **matplotlib/seaborn** | Visualization — training curves, confusion matrices, comparison charts |\n",
    "\n",
    "**Transfer Learning Ecosystem:**\n",
    "```\n",
    "torchvision.models\n",
    "├── ResNet50        → Skip connections, 25.6M params\n",
    "├── VGG16           → Deep sequential, 138M params\n",
    "├── EfficientNet_B0 → Compound scaling, 5.3M params\n",
    "└── MobileNet_V2    → Depthwise separable convs, 3.4M params\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "# --- PyTorch core ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Torchvision ---\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import (\n",
    "    ResNet50_Weights,\n",
    "    VGG16_Weights,\n",
    "    EfficientNet_B0_Weights,\n",
    "    MobileNet_V2_Weights,\n",
    ")\n",
    "\n",
    "# --- Model summary ---\n",
    "from torchinfo import summary\n",
    "\n",
    "# --- Data utilities ---\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler, Subset\n",
    "\n",
    "# --- Progress bars ---\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Standard libraries ---\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# --- Metrics ---\n",
    "from sklearn.metrics import classification_report, confusion_matrix, top_k_accuracy_score\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Constants & Device Setup\n",
    "\n",
    "### Key Differences from CNN-From-Scratch\n",
    "\n",
    "| Parameter | CNN From Scratch | Transfer Learning | Why? |\n",
    "|---|---|---|---|\n",
    "| `IMG_SIZE` | 128 | **224** | Pretrained models were trained on 224×224 ImageNet images |\n",
    "| `EPOCHS` | 25 | **15** | Pretrained features converge much faster |\n",
    "| `LEARNING_RATE` | 1e-3 | **1e-4** | Smaller LR to avoid destroying pretrained weights |\n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "We seed all random number generators to ensure consistent results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. CONSTANTS & DEVICE SETUP\n",
    "# ============================================================\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 15\n",
    "FEATURE_EXTRACT_EPOCHS = 5    # Phase 1: frozen backbone\n",
    "FINETUNE_EPOCHS = 10          # Phase 2: unfrozen backbone\n",
    "LEARNING_RATE = 1e-4\n",
    "FINETUNE_LR = 1e-5            # Lower LR for fine-tuning phase\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224                 # Standard for pretrained models\n",
    "NUM_CLASSES = 37\n",
    "SEED = 42\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "# --- Model names to compare ---\n",
    "MODEL_NAMES = [\"resnet50\", \"vgg16\", \"efficientnet_b0\", \"mobilenet_v2\"]\n",
    "\n",
    "# --- Reproducibility ---\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device selection ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "print(f\"\\nModels to compare: {MODEL_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Transforms\n",
    "\n",
    "### ImageNet Normalization\n",
    "\n",
    "Pretrained models were trained with specific normalization statistics computed from the ImageNet dataset. We **must** use the same values:\n",
    "\n",
    "$$x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "- **Mean:** [0.485, 0.456, 0.406] (per RGB channel)\n",
    "- **Std:** [0.229, 0.224, 0.225]\n",
    "\n",
    "Using different normalization would shift the input distribution, making the pretrained features meaningless.\n",
    "\n",
    "### Image Size: 224×224\n",
    "\n",
    "All four pretrained models were trained on 224×224 images. While they can technically handle other sizes (due to adaptive pooling), using the original training size gives the best feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. DATA TRANSFORMS\n",
    "# ============================================================\n",
    "\n",
    "# ImageNet normalization statistics (MUST match pretrained model training)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transforms — with augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation/Test transforms — deterministic\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(IMG_SIZE),  # 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "print(\"Training transforms:\")\n",
    "print(train_transforms)\n",
    "print(\"\\nValidation/Test transforms:\")\n",
    "print(val_test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset & DataLoader\n",
    "\n",
    "### Split Strategy\n",
    "\n",
    "Identical to Notebook 1 — we use the same data splits for a fair comparison:\n",
    "\n",
    "```\n",
    "Oxford-IIIT Pet Dataset\n",
    "├── trainval split (~3,680 images)\n",
    "│   ├── 80% → Training set  (~2,944 images)\n",
    "│   └── 20% → Validation set (~736 images)\n",
    "└── test split (~3,669 images) → Test set (as-is)\n",
    "```\n",
    "\n",
    "### TransformSubset Wrapper\n",
    "\n",
    "Same pattern as Notebook 1 — `random_split` returns `Subset` objects that don't support per-subset transforms, so we wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. DATASET & DATALOADER\n",
    "# ============================================================\n",
    "\n",
    "# --- TransformSubset wrapper ---\n",
    "class TransformSubset(Dataset):\n",
    "    \"\"\"Wraps a Subset to apply custom transforms instead of the parent's.\"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# --- Load Oxford-IIIT Pet dataset ---\n",
    "raw_trainval = torchvision.datasets.OxfordIIITPet(\n",
    "    root=\"./data\",\n",
    "    split=\"trainval\",\n",
    "    target_types=\"category\",\n",
    "    transform=None,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "raw_test = torchvision.datasets.OxfordIIITPet(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    target_types=\"category\",\n",
    "    transform=None,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# --- Split trainval into train (80%) and val (20%) ---\n",
    "trainval_size = len(raw_trainval)\n",
    "train_size = int(0.8 * trainval_size)\n",
    "val_size = trainval_size - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(raw_trainval, [train_size, val_size], generator=generator)\n",
    "\n",
    "# --- Apply transforms via wrapper ---\n",
    "train_dataset = TransformSubset(train_subset, transform=train_transforms)\n",
    "val_dataset = TransformSubset(val_subset, transform=val_test_transforms)\n",
    "test_dataset = TransformSubset(Subset(raw_test, range(len(raw_test))), transform=val_test_transforms)\n",
    "\n",
    "print(f\"Training set:   {len(train_dataset):,} images\")\n",
    "print(f\"Validation set: {len(val_dataset):,} images\")\n",
    "print(f\"Test set:       {len(test_dataset):,} images\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute class weights for WeightedRandomSampler ---\n",
    "train_labels = [raw_trainval[idx][1] for idx in train_subset.indices]\n",
    "\n",
    "class_counts = Counter(train_labels)\n",
    "num_samples = len(train_labels)\n",
    "class_weights = {cls: num_samples / count for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float64)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "# --- DataLoaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# --- Class names ---\n",
    "class_names = raw_trainval.classes\n",
    "\n",
    "print(f\"\\nNumber of batches — Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")\n",
    "print(f\"Class names ({len(class_names)} total): {class_names[:5]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualize Data\n",
    "\n",
    "Quick visual check to verify data loading and transforms are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. VISUALIZE DATA\n",
    "# ============================================================\n",
    "\n",
    "def denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    \"\"\"Reverse normalization for display.\"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "\n",
    "# --- Display a grid of sample images ---\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
    "fig.suptitle(\"Sample Images from Training Set (224x224)\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    image = denormalize(image)\n",
    "    image = image.permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(class_names[label], fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Class distribution bar chart ---\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "names = [class_names[c] for c, _ in sorted_classes]\n",
    "counts = [cnt for _, cnt in sorted_classes]\n",
    "\n",
    "ax.bar(range(len(names)), counts, color=\"steelblue\", edgecolor=\"white\")\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=90, fontsize=8)\n",
    "ax.set_ylabel(\"Number of Images\")\n",
    "ax.set_title(\"Class Distribution (Training Set)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.axhline(y=np.mean(counts), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(counts):.0f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Architectures\n",
    "\n",
    "### What is Transfer Learning?\n",
    "\n",
    "Transfer learning reuses a model trained on a **large source task** (ImageNet: 1.2M images, 1000 classes) for a **smaller target task** (our pet breeds: ~3,700 images, 37 classes). It works because:\n",
    "\n",
    "1. **Early layers learn universal features** — edges, textures, colors that are useful for ANY image task\n",
    "2. **Middle layers learn compositional features** — patterns like fur textures, eye shapes\n",
    "3. **Final layers learn task-specific features** — only these need retraining\n",
    "\n",
    "```\n",
    "Pretrained Model (ImageNet)\n",
    "┌─────────────────────────┐\n",
    "│  Early Layers           │ ← Universal features (edges, textures)\n",
    "│  (FROZEN — keep as-is)  │    These transfer well to any image task\n",
    "├─────────────────────────┤\n",
    "│  Middle Layers          │ ← Compositional features\n",
    "│  (FREEZE or FINE-TUNE)  │    May need slight adjustment\n",
    "├─────────────────────────┤\n",
    "│  Final Classifier       │ ← Task-specific (1000 ImageNet classes)\n",
    "│  (REPLACE with ours)    │    Replace with 37-class head\n",
    "└─────────────────────────┘\n",
    "```\n",
    "\n",
    "### Feature Extraction vs Fine-Tuning\n",
    "\n",
    "| Strategy | What Changes | When to Use |\n",
    "|---|---|---|\n",
    "| **Feature Extraction** | Only the new classifier head | Small dataset, similar to ImageNet |\n",
    "| **Fine-Tuning** | Classifier + some backbone layers | Moderate dataset, somewhat different from ImageNet |\n",
    "| **Full Fine-Tuning** | Everything | Large dataset, very different from ImageNet |\n",
    "\n",
    "We use a **two-phase approach**: feature extraction first (to train the new head), then fine-tuning (to adapt the backbone).\n",
    "\n",
    "### Our Four Models\n",
    "\n",
    "#### ResNet-50 (2015)\n",
    "- **Key innovation:** Skip connections (residual connections) that allow gradients to flow through shortcuts\n",
    "- **Architecture:** 50 layers with residual blocks: `x + F(x)` where `F` is 2-3 conv layers\n",
    "- **Why it works:** Skip connections solve the degradation problem — deeper networks can be trained without vanishing gradients\n",
    "- **Parameters:** 25.6M\n",
    "\n",
    "#### VGG-16 (2014)\n",
    "- **Key innovation:** Uniform architecture — all 3×3 convolutions, simple and deep\n",
    "- **Architecture:** 13 conv layers + 3 fully connected layers\n",
    "- **Trade-off:** Very large (138M params), but simple to understand\n",
    "- **Parameters:** 138M\n",
    "\n",
    "#### EfficientNet-B0 (2019)\n",
    "- **Key innovation:** Compound scaling — scales depth, width, and resolution together with a fixed ratio\n",
    "- **Architecture:** Mobile inverted bottleneck blocks (MBConv) with squeeze-and-excitation\n",
    "- **Why it works:** Achieves better accuracy with far fewer parameters than ResNet/VGG\n",
    "- **Parameters:** 5.3M\n",
    "\n",
    "#### MobileNet-V2 (2018)\n",
    "- **Key innovation:** Depthwise separable convolutions — factorize standard convolution into depthwise + pointwise\n",
    "- **Architecture:** Inverted residual blocks with linear bottlenecks\n",
    "- **Why it works:** Designed for mobile/edge deployment — minimal compute with competitive accuracy\n",
    "- **Parameters:** 3.4M\n",
    "\n",
    "### Modern Weights API\n",
    "\n",
    "We use torchvision's modern weights API (`ResNet50_Weights.DEFAULT`, etc.) instead of the deprecated `pretrained=True`. This provides:\n",
    "- Explicit weight versioning\n",
    "- Associated preprocessing transforms\n",
    "- Better reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. MODEL ARCHITECTURES\n",
    "# ============================================================\n",
    "\n",
    "def build_model(model_name, num_classes, freeze=True):\n",
    "    \"\"\"Build a pretrained model with a custom classifier head.\n",
    "\n",
    "    Args:\n",
    "        model_name: One of 'resnet50', 'vgg16', 'efficientnet_b0', 'mobilenet_v2'\n",
    "        num_classes: Number of output classes\n",
    "        freeze: If True, freeze the backbone (feature extraction mode)\n",
    "\n",
    "    Returns:\n",
    "        model: Modified pretrained model\n",
    "    \"\"\"\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        if freeze:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Replace final FC layer (originally 2048 → 1000)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    elif model_name == \"vgg16\":\n",
    "        model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        if freeze:\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Replace classifier (originally 25088 → 4096 → 4096 → 1000)\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    elif model_name == \"efficientnet_b0\":\n",
    "        model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        if freeze:\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Replace classifier (originally 1280 → 1000)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    elif model_name == \"mobilenet_v2\":\n",
    "        model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "        if freeze:\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Replace classifier (originally 1280 → 1000)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def unfreeze_last_n_layers(model, model_name, n=2):\n",
    "    \"\"\"Unfreeze the last n layers of the backbone for fine-tuning.\"\"\"\n",
    "    if model_name == \"resnet50\":\n",
    "        # Unfreeze layer4 (last residual block)\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif model_name == \"vgg16\":\n",
    "        # Unfreeze last 4 conv layers (features[24:])\n",
    "        for param in list(model.features.parameters())[-8:]:\n",
    "            param.requires_grad = True\n",
    "    elif model_name == \"efficientnet_b0\":\n",
    "        # Unfreeze last 2 MBConv blocks\n",
    "        for param in list(model.features.parameters())[-20:]:\n",
    "            param.requires_grad = True\n",
    "    elif model_name == \"mobilenet_v2\":\n",
    "        # Unfreeze last 3 inverted residual blocks\n",
    "        for param in list(model.features.parameters())[-20:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Count total and trainable parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "# --- Preview all models ---\n",
    "print(f\"{'Model':<20} {'Total Params':>15} {'Trainable (frozen)':>20}\")\n",
    "print(\"=\" * 60)\n",
    "for name in MODEL_NAMES:\n",
    "    m = build_model(name, NUM_CLASSES, freeze=True)\n",
    "    total, trainable = count_params(m)\n",
    "    print(f\"{name:<20} {total:>15,} {trainable:>20,}\")\n",
    "    del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Display Model Summary\n",
    "\n",
    "Showing ResNet-50 as a representative example — the skip connections and bottleneck blocks are clearly visible in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. DISPLAY MODEL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "example_model = build_model(\"resnet50\", NUM_CLASSES, freeze=True).to(DEVICE)\n",
    "summary(example_model, input_size=(1, 3, IMG_SIZE, IMG_SIZE), col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "del example_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Training Infrastructure\n",
    "\n",
    "### Two-Phase Training Strategy\n",
    "\n",
    "```\n",
    "Phase 1: Feature Extraction (5 epochs)\n",
    "┌────────────────────────────────────┐\n",
    "│  Backbone: FROZEN (no gradients)   │\n",
    "│  Classifier: TRAINING (lr=1e-4)   │\n",
    "│  Goal: Train the new head quickly  │\n",
    "└────────────────────────────────────┘\n",
    "            ↓\n",
    "Phase 2: Fine-Tuning (10 epochs)\n",
    "┌────────────────────────────────────┐\n",
    "│  Backbone: PARTIALLY UNFROZEN     │\n",
    "│  Last few layers: lr=1e-5         │\n",
    "│  Classifier: lr=1e-5             │\n",
    "│  Goal: Adapt backbone features    │\n",
    "└────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Differential Learning Rates\n",
    "\n",
    "During fine-tuning, we use a **lower learning rate** for backbone layers than for the classifier. This prevents the pretrained features from being destroyed while still allowing them to adapt:\n",
    "\n",
    "- Backbone layers: `1e-5` (gentle updates to pretrained weights)\n",
    "- Classifier head: `1e-5` (fine adjustments after Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. TRAINING INFRASTRUCTURE\n",
    "# ============================================================\n",
    "\n",
    "# Loss function (shared across all models)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def create_optimizer(model, lr):\n",
    "    \"\"\"Create Adam optimizer for trainable parameters only.\"\"\"\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return optim.Adam(trainable_params, lr=lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "def create_scheduler(optimizer):\n",
    "    \"\"\"Create ReduceLROnPlateau scheduler.\"\"\"\n",
    "    return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=2, factor=0.5, verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Training infrastructure ready.\")\n",
    "print(f\"Phase 1: Feature extraction — {FEATURE_EXTRACT_EPOCHS} epochs, LR={LEARNING_RATE}\")\n",
    "print(f\"Phase 2: Fine-tuning — {FINETUNE_EPOCHS} epochs, LR={FINETUNE_LR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Training & Validation Functions\n",
    "\n",
    "### Training Cycle\n",
    "\n",
    "Same forward-backward-update cycle as Notebook 1:\n",
    "```\n",
    "Forward → Loss → Backward → Update → Zero Grads\n",
    "```\n",
    "\n",
    "The key difference: during feature extraction, gradients only flow through the classifier head (backbone is frozen), so training is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. TRAINING & VALIDATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return running_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on validation/test data.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return running_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(model, loader, device):\n",
    "    \"\"\"Run inference and collect all predictions.\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "print(\"Training, validation, and test functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Train & Compare All Models\n",
    "\n",
    "### Comparison Methodology\n",
    "\n",
    "We train all four models with the **same data, same splits, same hyperparameters** — the only variable is the architecture. This ensures a fair comparison.\n",
    "\n",
    "For each model:\n",
    "1. **Phase 1 (Feature Extraction):** Freeze backbone, train classifier head for 5 epochs\n",
    "2. **Phase 2 (Fine-Tuning):** Unfreeze last backbone layers, train everything for 10 epochs with lower LR\n",
    "3. **Evaluate** on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. TRAIN & COMPARE ALL MODELS\n",
    "# ============================================================\n",
    "\n",
    "def train_model_two_phase(model_name, num_classes, train_loader, val_loader, device):\n",
    "    \"\"\"Train a model using two-phase transfer learning.\n",
    "\n",
    "    Phase 1: Feature extraction (frozen backbone)\n",
    "    Phase 2: Fine-tuning (partially unfrozen backbone)\n",
    "\n",
    "    Returns:\n",
    "        model: Trained model (best weights restored)\n",
    "        history: Dict with training metrics per epoch\n",
    "        total_time: Total training time in seconds\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {model_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"phase\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    total_start = time.time()\n",
    "\n",
    "    # ---- PHASE 1: Feature Extraction ----\n",
    "    print(f\"\\n--- Phase 1: Feature Extraction ({FEATURE_EXTRACT_EPOCHS} epochs) ---\")\n",
    "    model = build_model(model_name, num_classes, freeze=True).to(device)\n",
    "    optimizer = create_optimizer(model, LEARNING_RATE)\n",
    "    scheduler = create_scheduler(optimizer)\n",
    "\n",
    "    total_p, trainable_p = count_params(model)\n",
    "    print(f\"Trainable params: {trainable_p:,} / {total_p:,} ({100*trainable_p/total_p:.1f}%)\")\n",
    "\n",
    "    for epoch in range(1, FEATURE_EXTRACT_EPOCHS + 1):\n",
    "        epoch_start = time.time()\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"phase\"].append(1)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            marker = \" ★\"\n",
    "        else:\n",
    "            marker = \"\"\n",
    "\n",
    "        print(\n",
    "            f\"  [P1] Epoch {epoch}/{FEATURE_EXTRACT_EPOCHS} | \"\n",
    "            f\"Train: {train_loss:.4f} / {train_acc:.1f}% | \"\n",
    "            f\"Val: {val_loss:.4f} / {val_acc:.1f}% | \"\n",
    "            f\"{time.time()-epoch_start:.1f}s{marker}\"\n",
    "        )\n",
    "\n",
    "    # ---- PHASE 2: Fine-Tuning ----\n",
    "    print(f\"\\n--- Phase 2: Fine-Tuning ({FINETUNE_EPOCHS} epochs) ---\")\n",
    "    unfreeze_last_n_layers(model, model_name)\n",
    "    optimizer = create_optimizer(model, FINETUNE_LR)\n",
    "    scheduler = create_scheduler(optimizer)\n",
    "\n",
    "    total_p, trainable_p = count_params(model)\n",
    "    print(f\"Trainable params: {trainable_p:,} / {total_p:,} ({100*trainable_p/total_p:.1f}%)\")\n",
    "\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, FINETUNE_EPOCHS + 1):\n",
    "        epoch_start = time.time()\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"phase\"].append(2)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            marker = \" ★\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            marker = \"\"\n",
    "\n",
    "        print(\n",
    "            f\"  [P2] Epoch {epoch}/{FINETUNE_EPOCHS} | \"\n",
    "            f\"Train: {train_loss:.4f} / {train_acc:.1f}% | \"\n",
    "            f\"Val: {val_loss:.4f} / {val_acc:.1f}% | \"\n",
    "            f\"{time.time()-epoch_start:.1f}s{marker}\"\n",
    "        )\n",
    "\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\n  Total time: {total_time/60:.1f} min | Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return model, history, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train all models and collect results ---\n",
    "all_results = {}\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    # Train\n",
    "    model, history, train_time = train_model_two_phase(\n",
    "        model_name, NUM_CLASSES, train_loader, val_loader, DEVICE\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    print(f\"  Evaluating {model_name} on test set...\")\n",
    "    test_labels, test_preds, test_probs = test_model(model, test_loader, DEVICE)\n",
    "    top1_acc = 100.0 * np.mean(test_labels == test_preds)\n",
    "    top5_acc = 100.0 * top_k_accuracy_score(test_labels, test_probs, k=5)\n",
    "\n",
    "    total_p, trainable_p = count_params(model)\n",
    "\n",
    "    all_results[model_name] = {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"train_time\": train_time,\n",
    "        \"top1_acc\": top1_acc,\n",
    "        \"top5_acc\": top5_acc,\n",
    "        \"test_labels\": test_labels,\n",
    "        \"test_preds\": test_preds,\n",
    "        \"test_probs\": test_probs,\n",
    "        \"total_params\": total_p,\n",
    "    }\n",
    "\n",
    "    print(f\"  ✓ {model_name}: Top-1={top1_acc:.2f}%, Top-5={top5_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All models trained successfully!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Comprehensive Evaluation\n",
    "\n",
    "### Side-by-Side Comparison\n",
    "\n",
    "We compare all four models on:\n",
    "- **Test accuracy** (Top-1 and Top-5)\n",
    "- **Training time**\n",
    "- **Parameter count**\n",
    "- **Training curves** (loss and accuracy over epochs)\n",
    "- **Confusion matrix** (for the best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. COMPREHENSIVE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "# --- Comparison Table ---\n",
    "comparison_data = []\n",
    "for name in MODEL_NAMES:\n",
    "    r = all_results[name]\n",
    "    comparison_data.append({\n",
    "        \"Model\": name,\n",
    "        \"Top-1 Acc (%)\": f\"{r['top1_acc']:.2f}\",\n",
    "        \"Top-5 Acc (%)\": f\"{r['top5_acc']:.2f}\",\n",
    "        \"Total Params\": f\"{r['total_params']:,}\",\n",
    "        \"Train Time (min)\": f\"{r['train_time']/60:.1f}\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training curves for all models ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "colors = {\"resnet50\": \"#2196F3\", \"vgg16\": \"#FF9800\", \"efficientnet_b0\": \"#4CAF50\", \"mobilenet_v2\": \"#E91E63\"}\n",
    "\n",
    "for name in MODEL_NAMES:\n",
    "    h = all_results[name][\"history\"]\n",
    "    epochs = range(1, len(h[\"train_loss\"]) + 1)\n",
    "    c = colors[name]\n",
    "\n",
    "    # Train loss\n",
    "    axes[0, 0].plot(epochs, h[\"train_loss\"], \"-o\", color=c, label=name, markersize=3)\n",
    "    # Val loss\n",
    "    axes[0, 1].plot(epochs, h[\"val_loss\"], \"-o\", color=c, label=name, markersize=3)\n",
    "    # Train acc\n",
    "    axes[1, 0].plot(epochs, h[\"train_acc\"], \"-o\", color=c, label=name, markersize=3)\n",
    "    # Val acc\n",
    "    axes[1, 1].plot(epochs, h[\"val_acc\"], \"-o\", color=c, label=name, markersize=3)\n",
    "\n",
    "titles = [\"Train Loss\", \"Validation Loss\", \"Train Accuracy (%)\", \"Validation Accuracy (%)\"]\n",
    "for ax, title in zip(axes.flat, titles):\n",
    "    ax.set_title(title, fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Add phase separator line\n",
    "    ax.axvline(x=FEATURE_EXTRACT_EPOCHS + 0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Phase boundary\")\n",
    "\n",
    "plt.suptitle(\"Training History — All Models\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bar chart: Test accuracy comparison ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Top-1 Accuracy\n",
    "top1_scores = [all_results[n][\"top1_acc\"] for n in MODEL_NAMES]\n",
    "bar_colors = [colors[n] for n in MODEL_NAMES]\n",
    "bars1 = ax1.bar(MODEL_NAMES, top1_scores, color=bar_colors, edgecolor=\"white\", linewidth=1.5)\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax1.set_title(\"Top-1 Test Accuracy\", fontsize=14, fontweight=\"bold\")\n",
    "for bar, score in zip(bars1, top1_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f\"{score:.1f}%\",\n",
    "             ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Top-5 Accuracy\n",
    "top5_scores = [all_results[n][\"top5_acc\"] for n in MODEL_NAMES]\n",
    "bars2 = ax2.bar(MODEL_NAMES, top5_scores, color=bar_colors, edgecolor=\"white\", linewidth=1.5)\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Top-5 Test Accuracy\", fontsize=14, fontweight=\"bold\")\n",
    "for bar, score in zip(bars2, top5_scores):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f\"{score:.1f}%\",\n",
    "             ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.suptitle(\"Model Comparison — Test Set\", fontsize=15, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Best model: Confusion matrix + Classification report ---\n",
    "best_model_name = max(all_results, key=lambda n: all_results[n][\"top1_acc\"])\n",
    "best = all_results[best_model_name]\n",
    "\n",
    "print(f\"Best model: {best_model_name.upper()} (Top-1: {best['top1_acc']:.2f}%, Top-5: {best['top5_acc']:.2f}%)\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(best[\"test_labels\"], best[\"test_preds\"], target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix for best model ---\n",
    "cm = confusion_matrix(best[\"test_labels\"], best[\"test_preds\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(\n",
    "    cm, annot=False, fmt=\"d\", cmap=\"Blues\",\n",
    "    xticklabels=class_names, yticklabels=class_names, ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "ax.set_ylabel(\"Actual\", fontsize=12)\n",
    "ax.set_title(\n",
    "    f\"Confusion Matrix — {best_model_name.upper()} (Acc: {best['top1_acc']:.2f}%)\",\n",
    "    fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.yticks(rotation=0, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Save Best Model\n",
    "\n",
    "We save the best performing model's state dict along with metadata for easy loading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. SAVE BEST MODEL\n",
    "# ============================================================\n",
    "\n",
    "save_path = f\"dog_breed_transfer_{best_model_name}.pth\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_name\": best_model_name,\n",
    "    \"model_state_dict\": best[\"model\"].state_dict(),\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"class_names\": class_names,\n",
    "    \"top1_accuracy\": best[\"top1_acc\"],\n",
    "    \"top5_accuracy\": best[\"top5_acc\"],\n",
    "    \"comparison_results\": {\n",
    "        name: {\n",
    "            \"top1_acc\": all_results[name][\"top1_acc\"],\n",
    "            \"top5_acc\": all_results[name][\"top5_acc\"],\n",
    "            \"train_time\": all_results[name][\"train_time\"],\n",
    "            \"total_params\": all_results[name][\"total_params\"],\n",
    "        }\n",
    "        for name in MODEL_NAMES\n",
    "    },\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Best model ({best_model_name}) saved to '{save_path}'\")\n",
    "print(f\"Top-1 Accuracy: {best['top1_acc']:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {best['top5_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Bonus: Stanford Dogs Extension\n",
    "\n",
    "The **Stanford Dogs Dataset** (120 breeds, 20,580 images) is a much harder fine-grained classification task. Transfer learning is especially powerful here — the pretrained features can distinguish subtle breed differences that a from-scratch CNN would struggle with.\n",
    "\n",
    "### Expected Performance Boost\n",
    "\n",
    "| Dataset | CNN From Scratch | Transfer Learning |\n",
    "|---|---|---|\n",
    "| Oxford-IIIT Pet (37 classes) | ~30-40% | ~85-92% |\n",
    "| Stanford Dogs (120 classes) | ~10-20% | ~70-80% |\n",
    "\n",
    "> **Note:** This section requires a Kaggle API key or running on Kaggle where the dataset is available directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13. BONUS: STANFORD DOGS EXTENSION\n",
    "# ============================================================\n",
    "\n",
    "# Uncomment and run the cells below to train on Stanford Dogs (120 breeds).\n",
    "\n",
    "STANFORD_DOGS = False  # Set to True to enable\n",
    "\n",
    "if STANFORD_DOGS:\n",
    "    import os\n",
    "    from torchvision.datasets import ImageFolder\n",
    "\n",
    "    # --- Download dataset (Colab only — skip on Kaggle) ---\n",
    "    # !pip install kaggle -q\n",
    "    # !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
    "    # !kaggle dataset download -d jessicali9530/stanford-dogs-dataset -p ./data/stanford_dogs --unzip\n",
    "\n",
    "    # --- Configuration ---\n",
    "    STANFORD_NUM_CLASSES = 120\n",
    "\n",
    "    # --- Paths ---\n",
    "    # Kaggle: TRAIN_DIR = \"/kaggle/input/stanford-dogs-dataset/images/Images\"\n",
    "    TRAIN_DIR = \"./data/stanford_dogs/images/Images\"\n",
    "\n",
    "    # --- Load with ImageFolder ---\n",
    "    stanford_full = ImageFolder(root=TRAIN_DIR)\n",
    "    print(f\"Stanford Dogs: {len(stanford_full)} images, {len(stanford_full.classes)} classes\")\n",
    "\n",
    "    # --- Split: 80% train, 10% val, 10% test ---\n",
    "    total = len(stanford_full)\n",
    "    train_n = int(0.8 * total)\n",
    "    val_n = int(0.1 * total)\n",
    "    test_n = total - train_n - val_n\n",
    "\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "    sd_train, sd_val, sd_test = random_split(stanford_full, [train_n, val_n, test_n], generator=generator)\n",
    "\n",
    "    # Apply transforms\n",
    "    sd_train_dataset = TransformSubset(sd_train, transform=train_transforms)\n",
    "    sd_val_dataset = TransformSubset(sd_val, transform=val_test_transforms)\n",
    "    sd_test_dataset = TransformSubset(sd_test, transform=val_test_transforms)\n",
    "\n",
    "    # DataLoaders\n",
    "    sd_train_loader = DataLoader(sd_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    sd_val_loader = DataLoader(sd_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    sd_test_loader = DataLoader(sd_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    print(f\"Train: {len(sd_train_dataset)} | Val: {len(sd_val_dataset)} | Test: {len(sd_test_dataset)}\")\n",
    "\n",
    "    # --- Train best architecture on Stanford Dogs ---\n",
    "    print(f\"\\nTraining {best_model_name} on Stanford Dogs ({STANFORD_NUM_CLASSES} classes)...\")\n",
    "    sd_model, sd_history, sd_time = train_model_two_phase(\n",
    "        best_model_name, STANFORD_NUM_CLASSES, sd_train_loader, sd_val_loader, DEVICE\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    sd_labels, sd_preds, sd_probs = test_model(sd_model, sd_test_loader, DEVICE)\n",
    "    sd_top1 = 100.0 * np.mean(sd_labels == sd_preds)\n",
    "    sd_top5 = 100.0 * top_k_accuracy_score(sd_labels, sd_probs, k=5)\n",
    "    print(f\"\\nStanford Dogs Results — Top-1: {sd_top1:.2f}%, Top-5: {sd_top5:.2f}%\")\n",
    "else:\n",
    "    print(\"Stanford Dogs extension is disabled. Set STANFORD_DOGS = True to enable.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}