{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dog Breed Classification - CNN From Scratch\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/)\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/)\n\n---\n\n## Project Overview\n\nIn this notebook, we build a **Convolutional Neural Network (CNN) from scratch** using PyTorch to classify dog (and cat) breeds from the **Oxford-IIIT Pet Dataset** (37 categories, ~7,400 images). The goal is to deeply understand CNN mechanics — convolution, pooling, batch normalization, dropout — by designing every layer ourselves, without relying on pretrained weights.\n\n**What you'll learn:**\n- How convolutional neural networks process images\n- Data augmentation and normalization techniques\n- Handling class imbalance with weighted sampling\n- Training, validation, and evaluation pipelines in PyTorch\n- Interpreting metrics: accuracy, top-5 accuracy, confusion matrix, classification report\n\n---\n\n## Table of Contents\n\n1. [Imports](#1-imports)\n2. [Constants & Device Setup](#2-constants--device-setup)\n3. [Data Transforms](#3-data-transforms)\n4. [Dataset & DataLoader](#4-dataset--dataloader)\n5. [Visualize Data](#5-visualize-data)\n6. [Model Architecture](#6-model-architecture)\n7. [Display Model Summary](#7-display-model-summary)\n8. [Loss, Optimizer & Scheduler](#8-loss-optimizer--scheduler)\n9. [Training & Validation Functions](#9-training--validation-functions)\n10. [Training Loop](#10-training-loop)\n11. [Test & Evaluation](#11-test--evaluation)\n12. [Save Model](#12-save-model)\n13. [Bonus: Stanford Dogs Extension](#13-bonus-stanford-dogs-extension)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup — install required packages (silent for Colab/Kaggle)\n",
    "!pip install torchinfo -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Imports\n",
    "\n",
    "### Why These Libraries?\n",
    "\n",
    "| Library | Purpose |\n",
    "|---|---|\n",
    "| **torch** | Core deep learning framework — tensors, autograd, neural network modules |\n",
    "| **torch.nn** | Pre-built neural network layers (Conv2d, Linear, BatchNorm, etc.) |\n",
    "| **torch.nn.functional** | Stateless functions (activation functions, loss functions) |\n",
    "| **torch.optim** | Optimization algorithms (SGD, Adam, learning rate schedulers) |\n",
    "| **torchvision** | Computer vision utilities — datasets, transforms, pretrained models |\n",
    "| **torchinfo** | Model summary — parameter counts, layer shapes, memory usage |\n",
    "| **tqdm** | Progress bars for training loops |\n",
    "| **sklearn.metrics** | Classification report, confusion matrix, top-k accuracy |\n",
    "| **matplotlib/seaborn** | Visualization — training curves, confusion matrices, sample images |\n",
    "\n",
    "**PyTorch Ecosystem Overview:**\n",
    "```\n",
    "PyTorch Ecosystem\n",
    "├── torch           → Tensors, autograd, device management\n",
    "│   ├── torch.nn    → Layer definitions (Conv2d, Linear, ...)\n",
    "│   ├── torch.optim → Optimizers (Adam, SGD, schedulers)\n",
    "│   └── torch.utils.data → Dataset, DataLoader, samplers\n",
    "├── torchvision     → CV-specific tools\n",
    "│   ├── datasets    → Built-in datasets (ImageNet, CIFAR, Oxford Pet)\n",
    "│   ├── transforms  → Image preprocessing & augmentation\n",
    "│   └── models      → Pretrained architectures\n",
    "└── torchinfo       → Model introspection & summaries\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "# --- PyTorch core ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Torchvision ---\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --- Model summary ---\n",
    "from torchinfo import summary\n",
    "\n",
    "# --- Data utilities ---\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler, Subset\n",
    "\n",
    "# --- Progress bars ---\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Standard libraries ---\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# --- Metrics ---\n",
    "from sklearn.metrics import classification_report, confusion_matrix, top_k_accuracy_score\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Constants & Device Setup\n",
    "\n",
    "### Why Hyperparameters Matter\n",
    "\n",
    "Hyperparameters are the \"knobs\" we tune to control how the model learns. Unlike model parameters (weights/biases) which are learned during training, hyperparameters are set **before** training begins.\n",
    "\n",
    "| Hyperparameter | Our Value | Why? |\n",
    "|---|---|---|\n",
    "| `EPOCHS` | 25 | Enough iterations for a custom CNN to converge |\n",
    "| `LEARNING_RATE` | 1e-3 | Standard starting point for Adam optimizer |\n",
    "| `BATCH_SIZE` | 32 | Balances memory usage and gradient noise |\n",
    "| `IMG_SIZE` | 128 | Smaller than typical (224) to keep parameter count manageable for a from-scratch CNN |\n",
    "| `SEED` | 42 | Fixed seed for reproducibility across runs |\n",
    "\n",
    "### GPU vs CPU\n",
    "\n",
    "Deep learning involves massive matrix multiplications. GPUs have thousands of cores optimized for parallel computation, making them **10-100x faster** than CPUs for training. We use `torch.device` to automatically select the best available hardware:\n",
    "\n",
    "```\n",
    "Priority: CUDA (NVIDIA GPU) → MPS (Apple Silicon) → CPU\n",
    "```\n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "Setting random seeds ensures that results are reproducible across runs. We seed Python's `random`, NumPy, and PyTorch (both CPU and CUDA) to control all sources of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. CONSTANTS & DEVICE SETUP\n",
    "# ============================================================\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 37\n",
    "SEED = 42\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "# --- Reproducibility ---\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device selection ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Transforms\n",
    "\n",
    "### Why Normalize?\n",
    "\n",
    "Raw pixel values range from 0-255. Neural networks train faster and more stably when inputs are centered around 0 with unit variance. Normalization applies:\n",
    "\n",
    "$$x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu$ (mean) and $\\sigma$ (std) are computed per channel (R, G, B). We use ImageNet statistics as a reasonable default even for custom CNNs:\n",
    "- **Mean:** [0.485, 0.456, 0.406]\n",
    "- **Std:** [0.229, 0.224, 0.225]\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "Augmentation artificially increases training set diversity by applying random transformations. This acts as a **regularizer** — the model sees slightly different versions of each image every epoch, preventing it from memorizing specific pixel patterns.\n",
    "\n",
    "| Augmentation | What It Does | Why It Helps |\n",
    "|---|---|---|\n",
    "| `RandomHorizontalFlip` | Mirrors image left-right with 50% probability | Dogs can face either direction |\n",
    "| `RandomRotation(20)` | Rotates up to ±20 degrees | Photos aren't always perfectly level |\n",
    "| `ColorJitter` | Randomly adjusts brightness, contrast, saturation | Handles varying lighting conditions |\n",
    "| `RandomResizedCrop` | Crops a random portion and resizes | Forces model to recognize partial views |\n",
    "\n",
    "**Important:** Augmentation is applied **only during training**. Validation and test data use deterministic transforms (resize + center crop) so evaluation is consistent.\n",
    "\n",
    "```\n",
    "Training Pipeline:         Validation/Test Pipeline:\n",
    "┌─────────────┐            ┌─────────────┐\n",
    "│   Resize    │            │   Resize    │\n",
    "│  (144x144)  │            │  (144x144)  │\n",
    "├─────────────┤            ├─────────────┤\n",
    "│ RandomCrop  │            │ CenterCrop  │\n",
    "│  (128x128)  │            │  (128x128)  │\n",
    "├─────────────┤            ├─────────────┤\n",
    "│  RandomFlip │            │  ToTensor   │\n",
    "├─────────────┤            ├─────────────┤\n",
    "│  Rotation   │            │  Normalize  │\n",
    "├─────────────┤            └─────────────┘\n",
    "│ ColorJitter │\n",
    "├─────────────┤\n",
    "│  ToTensor   │\n",
    "├─────────────┤\n",
    "│  Normalize  │\n",
    "└─────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. DATA TRANSFORMS\n",
    "# ============================================================\n",
    "\n",
    "# ImageNet normalization statistics\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transforms — with augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((int(IMG_SIZE * 1.125), int(IMG_SIZE * 1.125))),  # 144x144\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation/Test transforms — deterministic, no augmentation\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((int(IMG_SIZE * 1.125), int(IMG_SIZE * 1.125))),  # 144x144\n",
    "    transforms.CenterCrop(IMG_SIZE),  # 128x128\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "print(\"Training transforms:\")\n",
    "print(train_transforms)\n",
    "print(\"\\nValidation/Test transforms:\")\n",
    "print(val_test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset & DataLoader\n",
    "\n",
    "### Dataset vs DataLoader\n",
    "\n",
    "| Concept | Role |\n",
    "|---|---|\n",
    "| **Dataset** | Stores samples and their labels. Defines `__getitem__` (get one sample) and `__len__` (total count). |\n",
    "| **DataLoader** | Wraps a Dataset to provide batching, shuffling, parallel loading, and sampling. |\n",
    "\n",
    "### Oxford-IIIT Pet Dataset\n",
    "\n",
    "- **37 categories** of pet breeds (25 dog breeds, 12 cat breeds)\n",
    "- **~7,400 images** total with roughly 200 images per class\n",
    "- Comes with two splits: `trainval` (~3,680 images) and `test` (~3,669 images)\n",
    "\n",
    "### Our Split Strategy\n",
    "\n",
    "```\n",
    "Oxford-IIIT Pet Dataset\n",
    "├── trainval split (~3,680 images)\n",
    "│   ├── 80% → Training set  (~2,944 images)\n",
    "│   └── 20% → Validation set (~736 images)\n",
    "└── test split (~3,669 images) → Test set (as-is)\n",
    "```\n",
    "\n",
    "This gives us approximately a **40/10/50** train/val/test ratio. The large test set provides a reliable estimate of real-world performance.\n",
    "\n",
    "### The `TransformSubset` Problem\n",
    "\n",
    "PyTorch's `random_split` returns `Subset` objects that inherit transforms from the parent dataset. But we need **different transforms** for training (with augmentation) vs validation (without). Our `TransformSubset` wrapper solves this by overriding the transform at access time.\n",
    "\n",
    "### Class Imbalance & Weighted Sampling\n",
    "\n",
    "If some breeds have more images than others, the model may become biased toward majority classes. `WeightedRandomSampler` assigns higher sampling probability to underrepresented classes, ensuring each class contributes equally to training:\n",
    "\n",
    "$$w_i = \\frac{1}{\\text{count}(\\text{class}_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. DATASET & DATALOADER\n",
    "# ============================================================\n",
    "\n",
    "# --- TransformSubset wrapper ---\n",
    "class TransformSubset(Dataset):\n",
    "    \"\"\"Wraps a Subset to apply custom transforms instead of the parent's.\"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# --- Load Oxford-IIIT Pet dataset (no transform yet — applied via TransformSubset) ---\n",
    "# Using target_type=\"category\" for breed classification (0-36)\n",
    "raw_trainval = torchvision.datasets.OxfordIIITPet(\n",
    "    root=\"./data\",\n",
    "    split=\"trainval\",\n",
    "    target_types=\"category\",\n",
    "    transform=None,   # Raw PIL images — transforms applied later\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "raw_test = torchvision.datasets.OxfordIIITPet(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    target_types=\"category\",\n",
    "    transform=None,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# --- Split trainval into train (80%) and val (20%) ---\n",
    "trainval_size = len(raw_trainval)\n",
    "train_size = int(0.8 * trainval_size)\n",
    "val_size = trainval_size - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(raw_trainval, [train_size, val_size], generator=generator)\n",
    "\n",
    "# --- Apply transforms via wrapper ---\n",
    "train_dataset = TransformSubset(train_subset, transform=train_transforms)\n",
    "val_dataset = TransformSubset(val_subset, transform=val_test_transforms)\n",
    "test_dataset = TransformSubset(Subset(raw_test, range(len(raw_test))), transform=val_test_transforms)\n",
    "\n",
    "print(f\"Training set:   {len(train_dataset):,} images\")\n",
    "print(f\"Validation set: {len(val_dataset):,} images\")\n",
    "print(f\"Test set:       {len(test_dataset):,} images\")\n",
    "print(f\"Total:          {len(train_dataset) + len(val_dataset) + len(test_dataset):,} images\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute class weights for WeightedRandomSampler ---\n",
    "# Extract labels from the training subset\n",
    "train_labels = [raw_trainval[idx][1] for idx in train_subset.indices]\n",
    "\n",
    "class_counts = Counter(train_labels)\n",
    "num_samples = len(train_labels)\n",
    "class_weights = {cls: num_samples / count for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float64)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "# --- DataLoaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# --- Class names ---\n",
    "class_names = raw_trainval.classes\n",
    "\n",
    "print(f\"\\nNumber of batches — Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")\n",
    "print(f\"\\nClass distribution (training set):\")\n",
    "for cls_idx in sorted(class_counts.keys())[:10]:\n",
    "    print(f\"  {class_names[cls_idx]:25s}: {class_counts[cls_idx]} images (weight: {class_weights[cls_idx]:.2f})\")\n",
    "print(f\"  ... ({len(class_counts)} classes total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualize Data\n",
    "\n",
    "### Why Visualize Before Modeling?\n",
    "\n",
    "Visualization helps us:\n",
    "1. **Verify data loading** — Are images loading correctly? Are labels right?\n",
    "2. **Understand the data** — What do the images look like? How varied are they?\n",
    "3. **Check augmentations** — Are transforms reasonable or too aggressive?\n",
    "4. **Spot issues** — Corrupted images, mislabeled data, class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. VISUALIZE DATA\n",
    "# ============================================================\n",
    "\n",
    "def denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    \"\"\"Reverse normalization for display.\"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "\n",
    "# --- Display a grid of sample images ---\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
    "fig.suptitle(\"Sample Images from Training Set\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    image = denormalize(image)\n",
    "    image = image.permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(class_names[label], fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Class distribution bar chart ---\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "names = [class_names[c] for c, _ in sorted_classes]\n",
    "counts = [cnt for _, cnt in sorted_classes]\n",
    "\n",
    "bars = ax.bar(range(len(names)), counts, color=\"steelblue\", edgecolor=\"white\")\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=90, fontsize=8)\n",
    "ax.set_ylabel(\"Number of Images\")\n",
    "ax.set_title(\"Class Distribution (Training Set)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.axhline(y=np.mean(counts), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(counts):.0f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Augmentation comparison: same image with different random transforms ---\n",
    "fig, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
    "fig.suptitle(\"Same Image with Different Augmentations\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Get a raw image from the underlying dataset\n",
    "raw_idx = train_subset.indices[0]\n",
    "raw_image, label = raw_trainval[raw_idx]\n",
    "\n",
    "# Show original\n",
    "axes[0].imshow(raw_image)\n",
    "axes[0].set_title(\"Original\", fontsize=10)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Show 5 augmented versions\n",
    "for i in range(1, 6):\n",
    "    augmented = train_transforms(raw_image)\n",
    "    augmented = denormalize(augmented).permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "    axes[i].imshow(augmented)\n",
    "    axes[i].set_title(f\"Augmented {i}\", fontsize=10)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Architecture\n",
    "\n",
    "### CNN Fundamentals\n",
    "\n",
    "A **Convolutional Neural Network** processes images by learning spatial hierarchies of features — edges in early layers, textures in middle layers, and object parts in deep layers.\n",
    "\n",
    "#### The Convolution Operation\n",
    "\n",
    "A convolutional layer slides a small filter (kernel) across the input, computing dot products at each position to produce a **feature map**:\n",
    "\n",
    "$$\\text{Output}(i, j) = \\sum_{m} \\sum_{n} \\text{Input}(i+m, j+n) \\cdot \\text{Kernel}(m, n) + \\text{bias}$$\n",
    "\n",
    "**Output size formula:**\n",
    "$$O = \\frac{W - K + 2P}{S} + 1$$\n",
    "\n",
    "where $W$ = input size, $K$ = kernel size, $P$ = padding, $S$ = stride.\n",
    "\n",
    "#### Key Building Blocks\n",
    "\n",
    "| Component | Purpose |\n",
    "|---|---|\n",
    "| **Conv2d** | Learns spatial features via learnable filters |\n",
    "| **BatchNorm2d** | Normalizes activations → faster, more stable training |\n",
    "| **ReLU** | Non-linear activation: $f(x) = \\max(0, x)$ — introduces non-linearity |\n",
    "| **MaxPool2d** | Downsamples by 2x → reduces spatial size, adds translation invariance |\n",
    "| **AdaptiveAvgPool2d** | Pools to a fixed size regardless of input dimensions |\n",
    "| **Dropout** | Randomly zeroes neurons during training → prevents co-adaptation |\n",
    "\n",
    "### Our Architecture\n",
    "\n",
    "```\n",
    "Input: (B, 3, 128, 128)\n",
    "│\n",
    "├── Conv Block 1: 3→32    │ (B, 32, 64, 64)\n",
    "│   Conv2d(3,32,3,pad=1) → BatchNorm → ReLU → MaxPool(2)\n",
    "│\n",
    "├── Conv Block 2: 32→64   │ (B, 64, 32, 32)\n",
    "│   Conv2d(32,64,3,pad=1) → BatchNorm → ReLU → MaxPool(2)\n",
    "│\n",
    "├── Conv Block 3: 64→128  │ (B, 128, 16, 16)\n",
    "│   Conv2d(64,128,3,pad=1) → BatchNorm → ReLU → MaxPool(2)\n",
    "│\n",
    "├── Conv Block 4: 128→256 │ (B, 256, 8, 8)\n",
    "│   Conv2d(128,256,3,pad=1) → BatchNorm → ReLU → MaxPool(2)\n",
    "│\n",
    "├── Conv Block 5: 256→512 │ (B, 512, 4, 4)\n",
    "│   Conv2d(256,512,3,pad=1) → BatchNorm → ReLU → MaxPool(2)\n",
    "│\n",
    "├── AdaptiveAvgPool2d(1)  │ (B, 512, 1, 1)\n",
    "├── Flatten               │ (B, 512)\n",
    "│\n",
    "├── Classifier:\n",
    "│   Linear(512, 256) → ReLU → Dropout(0.5)\n",
    "│   Linear(256, 128) → ReLU → Dropout(0.3)\n",
    "│   Linear(128, 37)  → (output logits)\n",
    "│\n",
    "Output: (B, 37) — raw class scores\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. MODEL ARCHITECTURE\n",
    "# ============================================================\n",
    "\n",
    "class DogBreedCNN(nn.Module):\n",
    "    \"\"\"Custom CNN for pet breed classification.\n",
    "\n",
    "    Architecture: 5 convolutional blocks + 3-layer classifier.\n",
    "    Each conv block: Conv2d → BatchNorm2d → ReLU → MaxPool2d\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature Extractor (Convolutional Blocks) ---\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 3 → 32 channels\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2: 32 → 64 channels\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3: 64 → 128 channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4: 128 → 256 channels\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 5: 256 → 512 channels\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # --- Global Average Pooling ---\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # --- Classifier ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)     # (B, 512, 4, 4)\n",
    "        x = self.pool(x)         # (B, 512, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # (B, 512)\n",
    "        x = self.classifier(x)   # (B, num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Instantiate model ---\n",
    "model = DogBreedCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Display Model Summary\n",
    "\n",
    "The `torchinfo` summary shows each layer's output shape, parameter count, and memory usage. This helps verify that our architecture is correct and understand where most parameters live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. DISPLAY MODEL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Loss, Optimizer & Scheduler\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "For multi-class classification, **Cross-Entropy Loss** measures the distance between the predicted probability distribution and the true label:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{c=1}^{C} y_c \\cdot \\log(\\hat{y}_c)$$\n",
    "\n",
    "where $y_c$ is the one-hot encoded true label and $\\hat{y}_c$ is the predicted probability for class $c$. PyTorch's `nn.CrossEntropyLoss` combines `LogSoftmax` + `NLLLoss` in one step for numerical stability.\n",
    "\n",
    "### Adam Optimizer\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) combines the best of two worlds:\n",
    "- **Momentum** (like SGD with momentum): Tracks exponential moving average of gradients\n",
    "- **RMSProp**: Adapts learning rate per-parameter based on gradient magnitude\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t$$\n",
    "\n",
    "We also add **weight decay** (L2 regularization) to penalize large weights and reduce overfitting.\n",
    "\n",
    "### Learning Rate Scheduler\n",
    "\n",
    "**ReduceLROnPlateau** monitors validation loss. If it stops improving for `patience` epochs, the learning rate is reduced by `factor`. This allows the model to take smaller steps as it approaches a minimum:\n",
    "\n",
    "```\n",
    "LR: 1e-3 ──plateau──→ 5e-4 ──plateau──→ 2.5e-4 ──plateau──→ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. LOSS, OPTIMIZER & SCHEDULER\n",
    "# ============================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=1e-4,  # L2 regularization\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",       # Monitor validation loss (lower is better)\n",
    "    patience=3,       # Wait 3 epochs before reducing\n",
    "    factor=0.5,       # Halve the learning rate\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau (patience=3, factor=0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Training & Validation Functions\n\n### The Training Cycle\n\nEach training step follows this cycle:\n\n```\n1. Forward Pass:   input → model → predictions\n2. Compute Loss:   predictions vs true labels → scalar loss\n3. Backward Pass:  loss.backward() → compute gradients (∂L/∂w for all weights)\n4. Update Weights: optimizer.step() → w_new = w_old - lr * gradient\n5. Zero Gradients: optimizer.zero_grad() → reset for next batch\n```\n\n### What `model.train()` vs `model.eval()` Does\n\n| Mode | Dropout | BatchNorm | Gradients |\n|---|---|---|---|\n| `model.train()` | Active (randomly zeros neurons) | Uses batch statistics | Computed |\n| `model.eval()` | Disabled | Uses running statistics | Not needed (use `torch.no_grad()`) |\n\n### Why Validate?\n\nValidation measures how well the model generalizes to data it hasn't been trained on. By comparing training and validation metrics, we can detect:\n\n| Scenario | Train Loss | Val Loss | Diagnosis |\n|---|---|---|---|\n| Both low, close together | ↓ | ↓ | Good fit |\n| Train low, val high | ↓↓ | ↑ | **Overfitting** — model memorized training data |\n| Both high | ↑ | ↑ | **Underfitting** — model too simple or needs more training |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 9. TRAINING & VALIDATION FUNCTIONS\n# ============================================================\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Train the model for one epoch.\n\n    Args:\n        model: The neural network\n        loader: Training DataLoader\n        criterion: Loss function\n        optimizer: Optimizer\n        device: torch.device\n\n    Returns:\n        avg_loss (float): Average loss over all batches\n        accuracy (float): Training accuracy (0-100)\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track metrics\n        running_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    avg_loss = running_loss / total\n    accuracy = 100.0 * correct / total\n    return avg_loss, accuracy\n\n\n@torch.no_grad()\ndef validate(model, loader, criterion, device):\n    \"\"\"Evaluate the model on validation/test data.\n\n    Args:\n        model: The neural network\n        loader: Validation/Test DataLoader\n        criterion: Loss function\n        device: torch.device\n\n    Returns:\n        avg_loss (float): Average loss\n        accuracy (float): Accuracy (0-100)\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in tqdm(loader, desc=\"Validating\", leave=False):\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        running_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    avg_loss = running_loss / total\n    accuracy = 100.0 * correct / total\n    return avg_loss, accuracy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Training Loop\n",
    "\n",
    "### Epoch-Level Training\n",
    "\n",
    "One **epoch** = one complete pass through the entire training set. We train for multiple epochs because:\n",
    "- The model needs repeated exposure to learn patterns\n",
    "- Each epoch uses different augmentations (via random transforms)\n",
    "- Gradients from a single pass aren't enough to converge\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Early stopping prevents overfitting by halting training when validation loss stops improving. If the validation loss doesn't decrease for `patience` consecutive epochs, we stop and restore the best model weights.\n",
    "\n",
    "```\n",
    "Epoch:  1  2  3  4  5  6  7  8  9  10\n",
    "Val Loss: ↓  ↓  ↓  ↓  ↑  ↑  ↑  ↑  ↑  STOP!\n",
    "                    ↑               ↑\n",
    "                Best model     Patience exhausted (5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "# --- History tracking ---\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"lr\": [],\n",
    "}\n",
    "\n",
    "# --- Early stopping variables ---\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    # Scheduler step (monitors val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Record history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        marker = \" ★ Best\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = \"\"\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch:02d}/{EPOCHS}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
    "        f\"LR: {current_lr:.6f} | Time: {epoch_time:.1f}s{marker}\"\n",
    "    )\n",
    "\n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch} epochs (patience={EARLY_STOPPING_PATIENCE})\")\n",
    "        break\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nRestored best model (val_loss={best_val_loss:.4f})\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nTotal training time: {total_time / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 11. Test & Evaluation\n\n### Final Evaluation on Unseen Data\n\nThe test set is data the model has **never** seen during training or validation. This gives us an unbiased estimate of real-world performance.\n\n### Metrics We Track\n\n| Metric | What It Measures |\n|---|---|\n| **Top-1 Accuracy** | % of times the model's top prediction is correct |\n| **Top-5 Accuracy** | % of times the correct class is in the model's top 5 predictions |\n| **Precision** | Of all predicted as class X, how many are actually X? |\n| **Recall** | Of all actual class X, how many did we correctly predict? |\n| **F1-Score** | Harmonic mean of precision and recall |\n| **Confusion Matrix** | NxN grid showing predicted vs actual for every class pair |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 11. TEST & EVALUATION\n# ============================================================\n\n@torch.no_grad()\ndef test_model(model, loader, device):\n    \"\"\"Run inference on the test set and collect all predictions.\n\n    Returns:\n        all_labels: Ground truth labels\n        all_preds: Predicted labels\n        all_probs: Predicted probabilities (for top-k accuracy)\n    \"\"\"\n    model.eval()\n    all_labels = []\n    all_preds = []\n    all_probs = []\n\n    for images, labels in tqdm(loader, desc=\"Testing\"):\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)\n        probs = F.softmax(outputs, dim=1)\n        _, predicted = outputs.max(1)\n\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n    return np.array(all_labels), np.array(all_preds), np.array(all_probs)\n\n\n# --- Run test ---\nprint(\"Evaluating on test set...\\n\")\ntest_labels, test_preds, test_probs = test_model(model, test_loader, DEVICE)\n\n# --- Accuracy ---\ntop1_acc = 100.0 * np.mean(test_labels == test_preds)\ntop5_acc = 100.0 * top_k_accuracy_score(test_labels, test_probs, k=5)\n\nprint(f\"Test Top-1 Accuracy: {top1_acc:.2f}%\")\nprint(f\"Test Top-5 Accuracy: {top5_acc:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Report ---\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(\n",
    "    cm, annot=False, fmt=\"d\", cmap=\"Blues\",\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "ax.set_ylabel(\"Actual\", fontsize=12)\n",
    "ax.set_title(f\"Confusion Matrix (Test Set) — Accuracy: {top1_acc:.2f}%\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.yticks(rotation=0, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Curves ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(epochs_range, history[\"train_loss\"], \"b-o\", label=\"Train Loss\", markersize=4)\n",
    "ax1.plot(epochs_range, history[\"val_loss\"], \"r-o\", label=\"Val Loss\", markersize=4)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Loss Curves\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(epochs_range, history[\"train_acc\"], \"b-o\", label=\"Train Acc\", markersize=4)\n",
    "ax2.plot(epochs_range, history[\"val_acc\"], \"r-o\", label=\"Val Acc\", markersize=4)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Accuracy Curves\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Training History — CNN From Scratch\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 12. Save Model\n\nWe save the model's **state dict** (learned weights and biases) rather than the entire model object. This is more portable and doesn't depend on the exact class definition being available at load time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 12. SAVE MODEL\n# ============================================================\n\nsave_path = \"dog_breed_cnn_from_scratch.pth\"\n\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"num_classes\": NUM_CLASSES,\n    \"img_size\": IMG_SIZE,\n    \"class_names\": class_names,\n    \"test_accuracy\": top1_acc,\n    \"history\": history,\n}, save_path)\n\nprint(f\"Model saved to '{save_path}'\")\nprint(f\"Test accuracy: {top1_acc:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 13. Bonus: Stanford Dogs Extension\n\nThe **Stanford Dogs Dataset** is a more challenging benchmark with **120 dog breeds** and **20,580 images**. This section shows how to adapt our CNN pipeline for this larger dataset.\n\n### Key Differences from Oxford-IIIT Pet\n\n| Property | Oxford-IIIT Pet | Stanford Dogs |\n|---|---|---|\n| Classes | 37 (dogs + cats) | 120 (dogs only) |\n| Images | ~7,400 | ~20,580 |\n| Difficulty | Moderate | Hard (fine-grained) |\n| Source | torchvision built-in | Kaggle download |\n\n> **Note:** This section requires a Kaggle API key. On Kaggle notebooks, the dataset is available directly. On Colab, you'll need to upload your `kaggle.json` credentials."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# 13. BONUS: STANFORD DOGS EXTENSION\n# ============================================================\n\n# Uncomment and run the cells below to train on Stanford Dogs (120 breeds).\n# This requires either:\n#   - Running on Kaggle (dataset available at /kaggle/input/stanford-dogs-dataset)\n#   - Kaggle API credentials for download\n\nSTANFORD_DOGS = False  # Set to True to enable\n\nif STANFORD_DOGS:\n    import os\n    from torchvision.datasets import ImageFolder\n\n    # --- Download dataset (Colab only — skip on Kaggle) ---\n    # !pip install kaggle -q\n    # !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n    # !kaggle dataset download -d jessicali9530/stanford-dogs-dataset -p ./data/stanford_dogs --unzip\n\n    # --- Configuration ---\n    STANFORD_NUM_CLASSES = 120\n    STANFORD_EPOCHS = 30\n\n    # --- Paths (adjust based on your environment) ---\n    # Kaggle:\n    # TRAIN_DIR = \"/kaggle/input/stanford-dogs-dataset/images/Images\"\n    # Colab/Local:\n    TRAIN_DIR = \"./data/stanford_dogs/images/Images\"\n\n    # --- Load with ImageFolder ---\n    stanford_full = ImageFolder(root=TRAIN_DIR)\n    print(f\"Stanford Dogs: {len(stanford_full)} images, {len(stanford_full.classes)} classes\")\n\n    # --- Split: 80% train, 10% val, 10% test ---\n    total = len(stanford_full)\n    train_n = int(0.8 * total)\n    val_n = int(0.1 * total)\n    test_n = total - train_n - val_n\n\n    generator = torch.Generator().manual_seed(SEED)\n    sd_train, sd_val, sd_test = random_split(stanford_full, [train_n, val_n, test_n], generator=generator)\n\n    # Apply transforms\n    sd_train_dataset = TransformSubset(sd_train, transform=train_transforms)\n    sd_val_dataset = TransformSubset(sd_val, transform=val_test_transforms)\n    sd_test_dataset = TransformSubset(sd_test, transform=val_test_transforms)\n\n    # DataLoaders\n    sd_train_loader = DataLoader(sd_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    sd_val_loader = DataLoader(sd_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    sd_test_loader = DataLoader(sd_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    print(f\"Train: {len(sd_train_dataset)} | Val: {len(sd_val_dataset)} | Test: {len(sd_test_dataset)}\")\n\n    # --- Build model with 120 classes ---\n    sd_model = DogBreedCNN(num_classes=STANFORD_NUM_CLASSES).to(DEVICE)\n    sd_criterion = nn.CrossEntropyLoss()\n    sd_optimizer = optim.Adam(sd_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    sd_scheduler = optim.lr_scheduler.ReduceLROnPlateau(sd_optimizer, patience=3, factor=0.5)\n\n    print(f\"\\nStanford Dogs model ready — {STANFORD_NUM_CLASSES} classes\")\n    print(\"Run the same training loop (Section 10) with sd_model, sd_train_loader, etc.\")\nelse:\n    print(\"Stanford Dogs extension is disabled. Set STANFORD_DOGS = True to enable.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}